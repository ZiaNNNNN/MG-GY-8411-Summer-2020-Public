{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leture 4\n",
    "\n",
    "In this demonstration, we want to study approaches to working with large datasets. While more data can mean more information for data mining, large datasets can pose challenges for us. We will explore some strategies to help us work with big data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing packages \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import sqlalchemy\n",
    "import pyspark\n",
    "\n",
    "import graphviz\n",
    "\n",
    "# changing some settings\n",
    "\n",
    "pd.set_option('display.max_rows', 10)\n",
    "pd.set_option('display.max_columns', 8)\n",
    "\n",
    "# indicating paths to files\n",
    "\n",
    "home = os.environ['HOME']\n",
    "path_litte_women = f\"{home}/shared/lecture-4/little_women.txt\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### File Size\n",
    "\n",
    "Remember that a bit represents the smallest unit of storage on a computer. Eight bits form a byte. We measure the size of files and folder with bytes.\n",
    "\n",
    "| Symbol\t| Prefix\t| Base 10 | Base 2\t|\n",
    "| -- | -- | -- | -- |\n",
    "K |\tkilo|\t$10^3$   |\t$2^{10}$ |\n",
    "M |\tmega | $10^6$  | $2^{20}$ |\n",
    "G\t| giga\t| $10^9$ | $2^{30}$ |\n",
    "T\t| tera\t| $10^{12}$ | $2^{40}$ |\n",
    "P\t| peta |\t$10^{15}$ | $2^{50}$ |\n",
    "\n",
    "We can use the command line interface to determine the size of files and folders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "little_women.txt\r\n"
     ]
    }
   ],
   "source": [
    "!ls ~/shared/lecture-4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have two files in the folder `shared/lecture-4/`. The command `ls` allows us to list the contents of a folder. Note that `~` indicates the root directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944\t/home/jovyan/shared/lecture-4/little_women.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du ~/shared/lecture-4/little_women.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `du` allows us to determine the size of files and folders. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "944K\t/home/jovyan/shared/lecture-4/little_women.txt\r\n"
     ]
    }
   ],
   "source": [
    "!du -sh ~/shared/lecture-4/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the flag `-sh` shows the prefix for the size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index of Table \n",
    "\n",
    "We can connect to the MySQL database with the `sqlalchemy` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sqlalchemy.create_engine('mysql+pymysql://dbreader:WuE8c1TF@mysql.jhub/cp126')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we need to specify the connection details\n",
    "\n",
    "> `<dialect_of_sql>+<driver_for_sql_in_python>://<user_name>:<password>@<ip_address>/<database_name>`\n",
    "\n",
    "Here `mysql.jhub` is an environment variable containing the IP address of the MySQL database. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR5DF6K1187B98F545</td>\n",
       "      <td>Movetron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR5DFHO1187B9A3CC4</td>\n",
       "      <td>3d5spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray / Fergie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray / Justin Timberlake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id                    artist_name\n",
       "0  AR5DF6K1187B98F545                       Movetron\n",
       "1  AR5DFHO1187B9A3CC4                         3d5spd\n",
       "2  AR5DHN51187B9B9363                      Macy Gray\n",
       "3  AR5DHN51187B9B9363             Macy Gray / Fergie\n",
       "4  AR5DHN51187B9B9363  Macy Gray / Justin Timberlake"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\" \n",
    "SELECT * \n",
    "FROM artists\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a table in the database called `artists` containing \n",
    "\n",
    "- `artist_id` : identifier of musical group\n",
    "- `artist_name` : primary key containing unique name of musical group\n",
    "\n",
    "Additionally we have a table in the database called `artists_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>artist_id</th>\n",
       "      <th>artist_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AR5DF6K1187B98F545</td>\n",
       "      <td>Movetron</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AR5DFHO1187B9A3CC4</td>\n",
       "      <td>3d5spd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray / Fergie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AR5DHN51187B9B9363</td>\n",
       "      <td>Macy Gray / Justin Timberlake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            artist_id                    artist_name\n",
       "0  AR5DF6K1187B98F545                       Movetron\n",
       "1  AR5DFHO1187B9A3CC4                         3d5spd\n",
       "2  AR5DHN51187B9B9363                      Macy Gray\n",
       "3  AR5DHN51187B9B9363             Macy Gray / Fergie\n",
       "4  AR5DHN51187B9B9363  Macy Gray / Justin Timberlake"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\" \n",
    "SELECT * \n",
    "FROM artists_index\n",
    "LIMIT 5;\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the tables `artists` and `artists_index` have the same records, the table `artists` has an index on the first 5 characters of the `artist_id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.6 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "query = \"\"\" \n",
    "SELECT COUNT(DISTINCT artist_id) AS Artists_Count\n",
    "FROM artists\n",
    "WHERE artist_name LIKE 'M%%' OR\n",
    "      artist_name LIKE 'm%%';\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `timeit` allows us to time the execution of code. Here `-n 1 -r 1` indicates that the code should run for 1 iteration with 1 repitition per iteration. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "\n",
    "query = \"\"\" \n",
    "SELECT COUNT(DISTINCT artist_id) AS Artists_Count\n",
    "FROM artists_index\n",
    "WHERE artist_name LIKE 'M%%' OR\n",
    "      artist_name LIKE 'm%%';\n",
    "\"\"\"\n",
    "\n",
    "pd.read_sql_query(query, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we have an improvement with an index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Reduce\n",
    "\n",
    "We have the text of the novel _Little Women_ by Louisa May Alcott in the `shared/lecture-4/` folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_litte_women, \"r\") as file_handle:\n",
    "    little_women_text = file_handle.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to count the occurence of words in the novel with map-reduce. We need three steps\n",
    "\n",
    "1. Map \n",
    "   * The mapper function generates key-value pairs. \n",
    "   * The keys are words and the values are the number 1\n",
    "2. Shuffle \n",
    "   * Group together key-value pairs based on the key\n",
    "3. Reduce\n",
    "   * Sum the values in each group of key-value pairs \n",
    "\n",
    "The `mapper` function takes a string a returns a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('big', 1),\n",
       " ('data', 1),\n",
       " ('analysis', 1),\n",
       " ('big', 1),\n",
       " ('data', 1),\n",
       " ('analysis', 1),\n",
       " ('analysis', 1),\n",
       " ('big', 1),\n",
       " ('business', 1),\n",
       " ('intelligence', 1)]"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mapper(string_document):\n",
    "    output = []\n",
    "    for word in string_document.split(\" \"):\n",
    "        output.append((word, 1))\n",
    "    \n",
    "    return output\n",
    "\n",
    "mapper(\"big data analysis big data analysis analysis big business intelligence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `shuffler` function takes a list of tuples and returns a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big': [('big', 1), ('big', 1), ('big', 1)],\n",
       " 'data': [('data', 1), ('data', 1)],\n",
       " 'analysis': [('analysis', 1), ('analysis', 1), ('analysis', 1)],\n",
       " 'business': [('business', 1)],\n",
       " 'intelligence': [('intelligence', 1)]}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def shuffler(list_pairs):\n",
    "    output = dict()\n",
    "    for pair in list_pairs:\n",
    "        old_value = output.get(pair[0], [])\n",
    "        new_value = old_value + [pair]\n",
    "        output.update({pair[0] : new_value})\n",
    "    \n",
    "    return output\n",
    "\n",
    "shuffler(mapper(\"big data analysis big data analysis analysis big business intelligence\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `reducer` function takes a dictionary and outputs a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'big': 3, 'data': 2, 'analysis': 3, 'business': 1, 'intelligence': 1}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def reducer(dictionary_words):\n",
    "    output = dict()\n",
    "    for key, value in dictionary_words.items():\n",
    "        output[key] = len(value)\n",
    "    \n",
    "    return output\n",
    "\n",
    "reducer(shuffler(mapper(\"big data analysis big data analysis analysis big business intelligence\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can count the words in the text of _Little Women_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer(shuffler(mapper(little_women_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark \n",
    "\n",
    "While we tend to use the Spark SQL component of Spark, we will use the Spark Core component of Spark for the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = pyspark.sql.SparkSession.builder.master(\"local\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to start a Spark session. Here we can configure the scheduler node and worker nodes. Since we will not work on a cluster like NYU Dumbo cluster, we can specify `local` to indicate our local computers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkContext = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Spark context is the interface to the Spark Core component of Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dependency Graph\n",
    "\n",
    "The `graphviz` package helps us to represent graphs. A graph is a collection of dots called nodes and lines called edges. The nodes are the entries of the graph and the lines are the relationships between entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 2.40.1 (20161225.0304)\n",
       " -->\n",
       "<!-- Title: %3 Pages: 1 -->\n",
       "<svg width=\"380pt\" height=\"131pt\"\n",
       " viewBox=\"0.00 0.00 379.78 131.00\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 127)\">\n",
       "<title>%3</title>\n",
       "<polygon fill=\"#ffffff\" stroke=\"transparent\" points=\"-4,4 -4,-127 375.7843,-127 375.7843,4 -4,4\"/>\n",
       "<!-- P -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>P</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"182.1424\" cy=\"-105\" rx=\"69.5877\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"182.1424\" y=\"-101.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Parent Node</text>\n",
       "</g>\n",
       "<!-- L -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>L</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"85.1424\" cy=\"-18\" rx=\"85.2851\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"85.1424\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Left Child Node</text>\n",
       "</g>\n",
       "<!-- P&#45;&gt;L -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>P&#45;&gt;L</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M162.513,-87.3943C148.0506,-74.4228 128.203,-56.6213 112.2314,-42.2962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"114.524,-39.651 104.7426,-35.5796 109.8501,-44.8621 114.524,-39.651\"/>\n",
       "<text text-anchor=\"middle\" x=\"175.6424\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Left Edge</text>\n",
       "</g>\n",
       "<!-- R -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>R</title>\n",
       "<ellipse fill=\"none\" stroke=\"#000000\" cx=\"280.1424\" cy=\"-18\" rx=\"91.784\" ry=\"18\"/>\n",
       "<text text-anchor=\"middle\" x=\"280.1424\" y=\"-14.3\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Right Child Node</text>\n",
       "</g>\n",
       "<!-- P&#45;&gt;R -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>P&#45;&gt;R</title>\n",
       "<path fill=\"none\" stroke=\"#000000\" d=\"M201.9742,-87.3943C216.5858,-74.4228 236.6379,-56.6213 252.7743,-42.2962\"/>\n",
       "<polygon fill=\"#000000\" stroke=\"#000000\" points=\"255.1855,-44.8359 260.3402,-35.5796 250.5382,-39.6011 255.1855,-44.8359\"/>\n",
       "<text text-anchor=\"middle\" x=\"276.6424\" y=\"-57.8\" font-family=\"Times,serif\" font-size=\"14.00\" fill=\"#000000\">Right Edge</text>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.dot.Digraph at 0x7f4c24574b50>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dot = graphviz.Digraph()\n",
    "\n",
    "dot.node('P', label = 'Parent Node')\n",
    "dot.node('L', label = 'Left Child Node')\n",
    "dot.node('R', label = 'Right Child Node')\n",
    "\n",
    "dot.edge('P', 'L', label = \"Left Edge\")\n",
    "dot.edge('P', 'R', label = \"Right Edge\")\n",
    "\n",
    "dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark uses distributed storage. If dataset does not fit into memory, then Spark partitions the dataset into pieces for storage on disk. If the dataset does not fit on disk, then Spark partitions the dataset into pieces for storage across a cluster. Any operation on the dataset becomes an operation on the chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "array = np.array([4,-1,2,17,-11,5,21,-7])\n",
    "\n",
    "chunks = np.array([[4,-1],[2,17],[-11,5],[21,-7]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark uses distributed computation. Using map-reduce, Spark breaks operations into dependent tasks spread across a cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-22"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks = 2 * chunks \n",
    "\n",
    "chunks.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can represent the tasks and the relationships between tasks in a dependency graph. The dependency graph helps Spark to improve execution of task through lazy evaluation and parellelism."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"graph-1.png\"  width=\"250\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark does not perform upstream calculations without dependent downstream calculations. In particular, Spark can cache unchanging values to focus on recalculating changing value. We call this lazy evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-154"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "array = np.array([4,-1,2,17,-11,5,21,-77])\n",
    "\n",
    "chunks = np.array([[4,-1],[2,17],[-11,5],[21,-77]])\n",
    "\n",
    "chunks = 2 * chunks \n",
    "\n",
    "chunks.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If tasks are not dependent, then Spark can execute them at the same time. So the Spark scheduler can distribute the job across a cluster. We call this parellelism.\n",
    "   \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resilient Distributed Datasets\n",
    "\n",
    "We can split the text in `little_women_text` into a list of strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "litte_women_rdd = sparkContext.parallelize(little_women_text.split(\" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We cna use the Spark context to partition the data into resilent distributed datasets. We can control the number of partitions with `repartition`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[16] at coalesce at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litte_women_rdd.repartition(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access entries of the RDD with `take`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['little', 'women', 'by', 'louisa']"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litte_women_rdd.take(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Spark uses map-reduce, we can take the same steps for counting the number of words in the novel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('little', 1), ('women', 1), ('by', 1)]"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litte_women_rdd_mapped = litte_women_rdd.map(lambda word: (word, 1))\n",
    "\n",
    "litte_women_rdd_mapped.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we do not have to shuffle the data. We can proceed to the reduce step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('little', 725), ('women', 46), ('by', 612)]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "litte_women_rdd_reduced = litte_women_rdd_mapped.reduceByKey(lambda a, b: a+b)\n",
    "\n",
    "litte_women_rdd_reduced.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can save the results to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "litte_women_rdd_reduced.repartition(1).saveAsTextFile(\"word_count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the contents of the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('little', 725)\r\n",
      "('women', 46)\r\n",
      "('by', 612)\r\n",
      "('louisa', 1)\r\n",
      "('may', 155)\r\n",
      "('alcott', 1)\r\n",
      "('contents', 2)\r\n",
      "('part', 54)\r\n",
      "('1', 1)\r\n",
      "('one', 723)\r\n"
     ]
    }
   ],
   "source": [
    "!head word_count/part-00000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mggy-8411]",
   "language": "python",
   "name": "conda-env-mggy-8411-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
